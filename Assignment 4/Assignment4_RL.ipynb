{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Assignment4_RL.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Assignment 4: Data Mining for Networks\n","\n","authors:\n","\n","Lynda Attouche ~  Lenny Klump\n","\n","\n","---\n","\n"],"metadata":{"id":"tEAiw70DL6p9"}},{"cell_type":"markdown","source":["#### Installing gym"],"metadata":{"id":"kVHHI7fAMGwO"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BObfeZl7_BGb","outputId":"bf51e5a1-2537-403a-a820-b4e0e67cdc03","executionInfo":{"status":"ok","timestamp":1645176805872,"user_tz":-60,"elapsed":3362,"user":{"displayName":"Lynda Att","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh_JmScmXomPPIygIneEflXvRZF2SZOy23ExWUi=s64","userId":"09343934939883857258"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: cmake in /usr/local/lib/python3.7/dist-packages (3.12.0)\n","Requirement already satisfied: gym[atari] in /usr/local/lib/python3.7/dist-packages (0.17.3)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (1.4.1)\n","Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scipy) (1.21.5)\n","Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[atari]) (1.3.0)\n","Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym[atari]) (1.5.0)\n","Requirement already satisfied: atari-py~=0.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[atari]) (0.2.9)\n","Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from gym[atari]) (4.1.2.30)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from gym[atari]) (7.1.2)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from atari-py~=0.2.0->gym[atari]) (1.15.0)\n","Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym[atari]) (0.16.0)\n"]}],"source":["!pip install cmake 'gym[atari]' scipy"]},{"cell_type":"markdown","source":["#### Imports"],"metadata":{"id":"FovvOla-_4Rg"}},{"cell_type":"code","source":["import numpy as np\n","import random\n","import gym.spaces\n","env = gym.make(\"Taxi-v3\")"],"metadata":{"id":"iOjM5KsT_C21","executionInfo":{"status":"ok","timestamp":1645176806497,"user_tz":-60,"elapsed":632,"user":{"displayName":"Lynda Att","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh_JmScmXomPPIygIneEflXvRZF2SZOy23ExWUi=s64","userId":"09343934939883857258"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["## Escape Game"],"metadata":{"id":"CSh9aOIuIMsD"}},{"cell_type":"markdown","source":["#### Defining states, actions, reward matrix and Qtable"],"metadata":{"id":"LzWHcLIlJzEy"}},{"cell_type":"code","source":["states = 6 #number of states(rooms)\n","actions = 6 #number of actions (moving from a room to another)\n","\n","#Defining the reward matrix\n","R = np.array([\n","              [-1.0,-1.0,-1.0,-1.0,0.0,-1.0],\n","              [-1.0,-1.0,-1.0,0.0,-1.0,100.0],\n","              [-1.0,-1.0,-1.0,0.0,-1.0,-1.0],\n","              [-1.0,0.0,0.0,-1.0,0.0,-1.0],\n","              [0.0,-1.0,-1.0,0.0,-1.0,100.0],\n","              [-1.0,0.0,-1.0,-1.0,0.0,100.0]\n","])\n","\n","#Initializing the Qtable\n","qtable = np.zeros((states,actions))\n","print(\"Q-table dim: \", qtable.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pDdRbRQ0IMGd","outputId":"fc4efde6-ac39-477b-e4c9-cddf92b25bb2","executionInfo":{"status":"ok","timestamp":1645176806498,"user_tz":-60,"elapsed":21,"user":{"displayName":"Lynda Att","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh_JmScmXomPPIygIneEflXvRZF2SZOy23ExWUi=s64","userId":"09343934939883857258"}}},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Q-table dim:  (6, 6)\n"]}]},{"cell_type":"markdown","source":["#### Hyperparameters"],"metadata":{"id":"38mv3VK6KZZl"}},{"cell_type":"code","source":["epsilon = 1.0 #Max greed (100%)\n","epsilon_min = 0.005 #Min greed (0.05%)\n","epsilon_decay = 0.99993 #decay multiplied with eps each episode\n","episodes = 400 #Num of games\n","max_steps = 100 #Max steps per episode\n","learning_rate = 0.65 #Learning rate \n","gamma = 0.8 #Discount factor"],"metadata":{"id":"wThNZWrHTQ-e","executionInfo":{"status":"ok","timestamp":1645176806499,"user_tz":-60,"elapsed":7,"user":{"displayName":"Lynda Att","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh_JmScmXomPPIygIneEflXvRZF2SZOy23ExWUi=s64","userId":"09343934939883857258"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["def q_learning(Q_table,epsilon, epsilon_min, epsilon_decay, episodes, max_steps, lr, gamma):\n","  \"\"\"\n","    Updates Q-table\n","    @params:\n","            - Q_table (ndarray) : initialized q-table \n","            - epsilon (float) : random number (to be used in epsilon-greedy search)\n","            - epsilon_min (float) : minimum value of epsilon\n","            - epsilon_decay (float) : used for the progressive decrease of epsilon\n","            - episodes (int): number of episodes of the game\n","            - max_steps (int) : maximum number of steps of an episode\n","            - lr (float) : learning rate\n","            - gamma (float) : discount factor\n","    @return:\n","            updated q-table (ndarray)\n","  \"\"\"\n","\n","  for episode in range(episodes):\n","    score = 0\n","    state = random.randint(0, 5)\n","\n","    for _ in range(max_steps):\n","\n","      invalid_move = True\n","      #Take best action in Q-table (exploitation)\n","      if np.random.uniform(0,1) > epsilon:\n","        action = np.argmax(Q_table[state,:])\n","      #Take random action (exploration)\n","      else:\n","        while invalid_move:\n","          rnd_action = random.randint(0,5)\n","          if R[state,:][rnd_action]!=-1: \n","            invalid_move = False\n","            action = rnd_action\n","\n","      #Take a step\n","      next_state = action\n","      reward = R[state,action]\n","\n","      #add up score\n","      score += reward\n","\n","      #Update Q-table \n","      Q_table[state,action] = reward + gamma*np.max(Q_table[next_state,:])\n","\n","      if state == 5: break\n","\n","      #Update state\n","      state = next_state\n","\n","    #Reducing epsilon each episode (exploration-exploitation trade-off)\n","    if epsilon >= epsilon_min: epsilon *= epsilon_decay\n"],"metadata":{"id":"zAnPGimarc1a","executionInfo":{"status":"ok","timestamp":1645176806501,"user_tz":-60,"elapsed":9,"user":{"displayName":"Lynda Att","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh_JmScmXomPPIygIneEflXvRZF2SZOy23ExWUi=s64","userId":"09343934939883857258"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["q_learning(qtable,epsilon, epsilon_min, epsilon_decay, episodes, max_steps, learning_rate, gamma)\n"],"metadata":{"id":"G9o6CdGysU0f","executionInfo":{"status":"ok","timestamp":1645176806849,"user_tz":-60,"elapsed":356,"user":{"displayName":"Lynda Att","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh_JmScmXomPPIygIneEflXvRZF2SZOy23ExWUi=s64","userId":"09343934939883857258"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["for i in range(6):\n","  print(qtable[i])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WAqCbXCpCgRv","outputId":"e62f48c2-ce0a-497b-9e4d-5e31561d608c","executionInfo":{"status":"ok","timestamp":1645176806850,"user_tz":-60,"elapsed":22,"user":{"displayName":"Lynda Att","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh_JmScmXomPPIygIneEflXvRZF2SZOy23ExWUi=s64","userId":"09343934939883857258"}}},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["[  0.   0.   0.   0. 400.   0.]\n","[  0.   0.   0. 320.   0. 500.]\n","[  0.   0.   0. 320.   0.   0.]\n","[  0. 400. 256.   0. 400.   0.]\n","[320.   0.   0. 320.   0. 500.]\n","[  0. 400.   0.   0. 400. 500.]\n"]}]},{"cell_type":"markdown","source":["## Taxi game"],"metadata":{"id":"izdIwXM-G0am"}},{"cell_type":"markdown","source":["#### Getting the state & action space and creating Q-Learning table"],"metadata":{"id":"sLWxCBv3_7dX"}},{"cell_type":"code","source":["state_space = env.observation_space.n\n","action_space = env.action_space.n\n","qtable_ = np.zeros((state_space,action_space))\n","print(\"Q-table dim: \",qtable_.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Us4SK5zv_G7X","outputId":"9f6c7d86-5535-4f2c-b42b-4cede64624f4","executionInfo":{"status":"ok","timestamp":1645176806850,"user_tz":-60,"elapsed":11,"user":{"displayName":"Lynda Att","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh_JmScmXomPPIygIneEflXvRZF2SZOy23ExWUi=s64","userId":"09343934939883857258"}}},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Q-table dim:  (500, 6)\n"]}]},{"cell_type":"markdown","source":["#### Defining hyperparameters"],"metadata":{"id":"8HIWWwsbAUD1"}},{"cell_type":"code","source":["#same as the previous hyperparameters except gamma and number of episodes\n","episodes_ = 50000 #Num of games\n","gamma_ = 0.65 #Discount factor"],"metadata":{"id":"gqb4y5lV_Nke","executionInfo":{"status":"ok","timestamp":1645176806851,"user_tz":-60,"elapsed":9,"user":{"displayName":"Lynda Att","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh_JmScmXomPPIygIneEflXvRZF2SZOy23ExWUi=s64","userId":"09343934939883857258"}}},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":["#### Defining Q_Learning algorithm"],"metadata":{"id":"XRRyEP2BAaAU"}},{"cell_type":"code","source":["def q_learning(qtable,epsilon, epsilon_min, epsilon_decay, episodes, max_steps, lr, gamma):\n","    \"\"\"\n","    Updates Q-table\n","    @params:\n","            - Q_table (ndarray) : initialized q-table \n","            - epsilon (float) : random number (to be used in epsilon-greedy search)\n","            - epsilon_min (float) : minimum value of epsilon\n","            - epsilon_decay (float) : used for the progressive decrease of epsilon\n","            - episodes (int): number of episodes of the game\n","            - max_steps (int) : maximum number of steps of an episode\n","            - lr (float) : learning rate\n","            - gamma (float) : discount factor\n","    @return:\n","            updated q-table (ndarray)\n","  \"\"\"\n","\n","    for episode in range(episodes):\n","        #Reset the game parameters\n","        state = env.reset() #current state\n","        done = False #to stop game\n","        score = 0\n","        for _ in range(max_steps):\n","            #Take best action in Q-table \n","            #exploitation:\n","            if np.random.uniform(0,1) > epsilon:\n","                action = np.argmax(qtable[state,:])\n","            #Take random action \n","            #exploration:\n","            else:\n","                action = env.action_space.sample()\n","\n","            #stem the game forward\n","            next_state, reward, done, _ = env.step(action)\n","\n","            #add up score\n","            score += reward\n","\n","            #Update Q-table with new Qval\n","            qtable[state,action] = (1-learning_rate)*qtable[state,action] + learning_rate*(reward+gamma*np.max(qtable[next_state,:]))\n","\n","            #Update state\n","            state = next_state\n","\n","            if done:\n","              break\n","\n","        #Reducing epsilon each episode (exploration-exploitation trade-off)\n","        if epsilon >= epsilon_min:\n","             epsilon *= epsilon_decay"],"metadata":{"id":"v5xMGywE_P3u","executionInfo":{"status":"ok","timestamp":1645176806851,"user_tz":-60,"elapsed":9,"user":{"displayName":"Lynda Att","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh_JmScmXomPPIygIneEflXvRZF2SZOy23ExWUi=s64","userId":"09343934939883857258"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["q_learning(qtable_, epsilon, epsilon_min, epsilon_decay, episodes_, max_steps, learning_rate, gamma_)"],"metadata":{"id":"3i1CC3eV_VIu","executionInfo":{"status":"ok","timestamp":1645176855104,"user_tz":-60,"elapsed":48262,"user":{"displayName":"Lynda Att","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh_JmScmXomPPIygIneEflXvRZF2SZOy23ExWUi=s64","userId":"09343934939883857258"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["qtable_"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"j724lz_l_bBV","outputId":"10240840-ca0e-4cc9-f102-3c113bd13949","executionInfo":{"status":"ok","timestamp":1645176855105,"user_tz":-60,"elapsed":26,"user":{"displayName":"Lynda Att","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh_JmScmXomPPIygIneEflXvRZF2SZOy23ExWUi=s64","userId":"09343934939883857258"}}},"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[  0.        ,   0.        ,   0.        ,   0.        ,\n","          0.        ,   0.        ],\n","       [ -2.65712496,  -2.54942301,  -2.65712496,  -2.54942301,\n","         -2.38372771, -11.54942301],\n","       [ -1.73663363,  -1.1332825 ,  -1.73663363,  -1.1332825 ,\n","         -0.20505   , -10.1332825 ],\n","       ...,\n","       [ -0.20505   ,   1.223     ,  -0.20505   ,  -1.1332825 ,\n","         -9.20505   ,  -9.20505   ],\n","       [ -2.38372771,  -2.12881186,  -2.38372771,  -2.12881186,\n","        -11.38372771, -11.38372771],\n","       [  6.8       ,   3.42      ,   6.8       ,  12.        ,\n","         -2.2       ,  -2.2       ]])"]},"metadata":{},"execution_count":12}]},{"cell_type":"markdown","source":["#### Comparing with a random agent\n"],"metadata":{"id":"hwO4VuO-IEEV"}},{"cell_type":"markdown","source":["Concerning the random agent, it doesn't consider the state of our environment to select actions. They are selected randomly, that is why we call it random agent. \n","\n","\n","We compare the Q-learning agent and the random agent according to the number of penaties and timesteps per episode and the number of rewards per action. "],"metadata":{"id":"lsNdUtNL43Ns"}},{"cell_type":"code","source":["def eval_qlAgent(qtable,episodes, max_steps):\n","  \"\"\"\n","    Evaluates Q-learning agent\n","    @params:\n","            - Q_table (ndarray) : initialized q-table \n","            - episodes (int): number of episodes of the game\n","            - max_steps (int) : maximum number of steps of an episode\n","    @return:\n","            - average number of timesteps (float) , average number of penalties (float)\n","  \"\"\"\n","\n","\n","  total_epochs, total_penalties = 0, 0 #contrains respectively the number of epochs and penalties at the end\n","\n","  for episode in range(episodes):\n","    #Reset the game parameters\n","    state = env.reset()\n","    done = False\n","    score = 0\n","    epochs, penalties = 0, 0 #contrains respectively the number of epochs and penalties for each episode\n","\n","    for _ in range(max_steps):\n","      \n","      #we select the best action from the Q-table \n","      action = np.argmax(qtable[state,:])\n","\n","      #Take a step\n","      next_state, reward, done, _ = env.step(action)\n","      \n","      if reward == -10:\n","        penalties+=1\n","\n","      #add up score\n","      score += reward\n","\n","      #Update state\n","      state = next_state\n","\n","      epochs+=1\n","\n","\n","      if done:\n","        break\n","    \n","    total_penalties += penalties\n","    total_epochs += epochs\n","  \n","  return total_epochs / episodes, total_penalties / episodes "],"metadata":{"id":"VFXKh-ZXBElT","executionInfo":{"status":"ok","timestamp":1645176855106,"user_tz":-60,"elapsed":16,"user":{"displayName":"Lynda Att","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh_JmScmXomPPIygIneEflXvRZF2SZOy23ExWUi=s64","userId":"09343934939883857258"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["def eval_randomAgent(episodes, max_steps):\n","  \"\"\"\n","    Evaluates Random Agent\n","    @params:\n","            - episodes (int): number of episodes of the game\n","            - max_steps (int) : maximum number of steps of an episode\n","    @return:\n","            - average number of timesteps (float) , average number of penalties (float)\n","  \"\"\"\n","\n","  \n","  total_epochs, total_penalties = 0, 0 #contrains respectively the number of epochs and penalties at the end\n","  \n","  for episode in range(episodes):\n","    state = env.reset()\n","    done = False\n","    score = 0\n","    epochs, penalties = 0, 0 #contrains respectively the number of epochs and penalties for each episode\n","\n","    for _ in range(max_steps):\n","      #As we said previously the agent select a random action as follows:\n","      action = env.action_space.sample()\n","\n","      #Take a step\n","      next_state, reward, done, _ = env.step(action)\n","      \n","      \n","      if reward == -10:\n","        penalties+=1\n","\n","\n","      #add up score\n","      score += reward\n","\n","      #Update state\n","      state = next_state\n","\n","      epochs+=1\n","\n","\n","      if done:\n","        break\n","        \n","    total_penalties += penalties\n","    total_epochs += epochs\n","  \n","  return total_epochs / episodes, total_penalties / episodes "],"metadata":{"id":"Nu9zKWlJ8Hzx","executionInfo":{"status":"ok","timestamp":1645176855107,"user_tz":-60,"elapsed":16,"user":{"displayName":"Lynda Att","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh_JmScmXomPPIygIneEflXvRZF2SZOy23ExWUi=s64","userId":"09343934939883857258"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["t1,p1 = eval_qlAgent(qtable_,episodes_, max_steps)\n","t2,p2 = eval_randomAgent(episodes, max_steps)\n","\n","print(f\"Results after {episodes_} episodes:\")\n","print(\"\")\n","print(\"Average timesteps per episode:\")\n","print(\"     Q-learning agent:\",t1)\n","print(\"     Random agent:\",t2)\n","print(\"\")\n","print(\"Average penalties per episode:\")\n","print(\"     Q-learning agent:\",p1)\n","print(\"     Random agent:\",p2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LdyPxxdg9Riy","outputId":"938b4df7-0194-4327-c685-bd19e98b83ac","executionInfo":{"status":"ok","timestamp":1645176866189,"user_tz":-60,"elapsed":11097,"user":{"displayName":"Lynda Att","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh_JmScmXomPPIygIneEflXvRZF2SZOy23ExWUi=s64","userId":"09343934939883857258"}}},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Results after 50000 episodes:\n","\n","Average timesteps per episode:\n","     Q-learning agent: 13.07462\n","     Random agent: 99.82\n","\n","Average penalties per episode:\n","     Q-learning agent: 0.0\n","     Random agent: 32.2925\n"]}]},{"cell_type":"markdown","source":["From these results, it can be seen how much more effective the Q-learning agent is than the random agent. Indeed, we have that:\n","\n","*   The random agent performs a greater number of times than the Q-learning agent. This corresponds to a percentage equivalent to 761.8%\n","\n","*   The Q-learning agent did not commit any penalty, which is the ideal and shows its performance. On the other hand, the random agent committed a fair number of penalties.\n"],"metadata":{"id":"JSO2LKSG_4M0"}}]}